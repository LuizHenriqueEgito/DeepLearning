{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2600ede9-0720-4cbe-b3ac-be5ad9f22cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742e57ca-ae5f-4c45-ac58-9c906bc68180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46350e1e-7804-4008-9881-b51b584714e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e3fad7-a44f-4650-93ee-2c5b26059bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_loop import eval_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a81a480-7d4d-4b73-8c90-e99f711dceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de GPUs disponíveis: 1\n",
      "--- GPU 0 ---\n",
      "Nome: Tesla T4\n",
      "Memória total: 14.75 GB\n",
      "Memória disponível: 0.00 GB\n",
      "Memória reservada: 0.00 GB\n",
      "Capacidade de Computação: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar o número de GPUs disponíveis\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Número de GPUs disponíveis: {num_gpus}\")\n",
    "\n",
    "# Obter informações detalhadas sobre cada GPU\n",
    "for i in range(num_gpus):\n",
    "    print(f\"--- GPU {i} ---\")\n",
    "    print(f\"Nome: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Memória total: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"Memória disponível: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n",
    "    print(f\"Memória reservada: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n",
    "    print(f\"Capacidade de Computação: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4660659-bd52-4878-bb38-a46656d35cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'DEVICE: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9f35c8-4414-444d-b683-30a744a3c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Acessar os dados\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Exibir informações sobre o conjunto de dados\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa90e902-9343-4d39-b932-7afba9cbb7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the text\n",
    "train_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4280f7de-03d3-44bc-a9d9-634fb7a7783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average of texts\n",
    "mean_words_text = np.mean(\n",
    "    list(map(lambda x: len(x.split()), train_data['text']))\n",
    ")\n",
    "\n",
    "std_words_text = np.std(\n",
    "    list(map(lambda x: len(x.split()), train_data['text']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756ff488-763a-47c9-86dd-2a0eb91d2061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de palavras por texto: 134.10.\n",
      "Desvio padrão de palavras por texto: 121.40.\n"
     ]
    }
   ],
   "source": [
    "print(f'Média de palavras por texto: {mean_words_text:.2f}.')\n",
    "print(f'Desvio padrão de palavras por texto: {std_words_text:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b3e1d4b-8878-49ba-9a0a-a32b520df589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6912763e-8927-4ea7-a1fe-3d6b348346d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: \n",
      "After waiting for almost 30 minutes to trade in an old phone part of the buy back program, our customer service rep incorrectly processed the transaction. This led to us waiting another 30 minutes for him to correct it. Don't visit this store if you want pleasant or good service.\n",
      "TOKENS: \n",
      "['after', 'waiting', 'for', 'almost', '30', 'minutes', 'to', 'trade', 'in', 'an', 'old', 'phone', 'part', 'of', 'the', 'buy', 'back', 'program', ',', 'our', 'customer', 'service', 'rep', 'incorrectly', 'processed', 'the', 'transaction', '.', 'this', 'led', 'to', 'us', 'waiting', 'another', '30', 'minutes', 'for', 'him', 'to', 'correct', 'it', '.', 'don', \"'\", 't', 'visit', 'this', 'store', 'if', 'you', 'want', 'pleasant', 'or', 'good', 'service', '.']\n",
      "TOKENS IDS: \n",
      "[101, 2044, 3403, 2005, 2471, 2382, 2781, 2000, 3119, 1999, 2019, 2214, 3042, 2112, 1997, 1996, 4965, 2067, 2565, 1010, 2256, 8013, 2326, 16360, 19721, 13995, 1996, 12598, 1012, 2023, 2419, 2000, 2149, 3403, 2178, 2382, 2781, 2005, 2032, 2000, 6149, 2009, 1012, 2123, 1005, 1056, 3942, 2023, 3573, 2065, 2017, 2215, 8242, 2030, 2204, 2326, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = train_data['text'][13]\n",
    "print(f'TEXT: \\n{text}')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f'TOKENS: \\n{tokens}')\n",
    "tokens_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(f'TOKENS IDS: \\n{tokens_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e93d51-ded0-4e4e-8e0e-017531aafab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256  # nº de tokens de entrada do modelo\n",
    "D_MODEL = 512  # nº de dimensões de embedding\n",
    "N_HEADS = 8  # nº de cabeças utilizadas no multi-head attention\n",
    "Nx = 6  # nº de vezes que é repassado no multi-head attention\n",
    "N_OUTPUT = 5  # nº de classes de saida\n",
    "VOCAB_SIZE = tokenizer.vocab_size  # vocab size\n",
    "LR = 1e-5  # Learning Rate\n",
    "BATCH_SIZE = 64  # Batch Size\n",
    "EPOCHS = 20  # épocas de trainamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27215228-097f-42c3-a4cd-b9f2a52b17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewFullDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data, \n",
    "        tokenizer: Tokenizer = tokenizer, \n",
    "        seq_len: int = SEQ_LEN\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.num_rows\n",
    "        \n",
    "    def __getitem__(self, id_i) -> dict[int, list[int]]:\n",
    "        item = self.data[id_i]\n",
    "        label, text = item['label'], item['text']\n",
    "        tokens_list = tokenizer.encode(\n",
    "            text,\n",
    "            max_length=self.seq_len,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',  # Adiciona padding até max_length\n",
    "            return_tensors='pt' \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'label': label, \n",
    "            'tokens': tokens_list.squeeze(0), \n",
    "            'text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe03329-3dc1-4778-9ef5-a0d5fff4b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YelpReviewFullDataset(train_data)\n",
    "test_dataset = YelpReviewFullDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d39a7e-0865-48ea-9dfe-14122c5b960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7923c6b-1c80-4532-9a8e-353fb2d42377",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac34b8-b8db-4e37-9adb-05a10a4d3776",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4fd9081-7f5a-4a09-84f3-e4a2afc570b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.register_buffer(\"pe\", self._create_positional_encoding(self.seq_len))\n",
    "\n",
    "    def _create_positional_encoding(self, seq_len):\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2).float()\n",
    "            * (-math.log(10000.0) / self.d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:, :seq_len, :].to(x.device)\n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcc5f249-06cf-4d56-8e08-de3b66617fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YepReviewModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int = SEQ_LEN,\n",
    "        d_model: int = D_MODEL,\n",
    "        vocab_size: int = VOCAB_SIZE,\n",
    "        num_heads: int = N_HEADS,\n",
    "        n_x: int = Nx,\n",
    "        dropout: float = 0.1,\n",
    "        n_outputs: int = N_OUTPUT\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # configurações do modelo\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.n_x = n_x\n",
    "        self.dropout = dropout\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        # componentes\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, \n",
    "            embedding_dim=self.d_model, \n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.d_model, dropout=self.dropout, seq_len=self.seq_len)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model, \n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout, \n",
    "            norm_first=True, \n",
    "            batch_first=True,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=self.n_x)\n",
    "        \n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(8, self.n_outputs)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.linear_layer:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder_block(x)\n",
    "        # Pegando a representação vetorial do token <CLS>\n",
    "        x = x[:, 0, :]\n",
    "        x = self.linear_layer(x)\n",
    "        # x = F.layer_norm(x, x.size()[1:])\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec610bc9-eafd-4339-bea3-150a2bf89858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = YepReviewModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c3dcc0-5c8d-47f1-805f-5dc66a231ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/yepreview_model_.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7975e3b-0edd-41fa-8b3a-9c15e6f57b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "YepReviewModel                                                    --\n",
       "├─Embedding: 1-1                                                  15,627,264\n",
       "├─PositionalEncoding: 1-2                                         --\n",
       "│    └─Dropout: 2-1                                               --\n",
       "├─TransformerEncoderLayer: 1-3                                    --\n",
       "│    └─MultiheadAttention: 2-2                                    787,968\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1                  262,656\n",
       "│    └─Linear: 2-3                                                1,050,624\n",
       "│    └─Dropout: 2-4                                               --\n",
       "│    └─Linear: 2-5                                                1,049,088\n",
       "│    └─LayerNorm: 2-6                                             1,024\n",
       "│    └─LayerNorm: 2-7                                             1,024\n",
       "│    └─Dropout: 2-8                                               --\n",
       "│    └─Dropout: 2-9                                               --\n",
       "├─TransformerEncoder: 1-4                                         --\n",
       "│    └─ModuleList: 2-10                                           --\n",
       "│    │    └─TransformerEncoderLayer: 3-2                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-3                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-4                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-5                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-6                          3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-7                          3,152,384\n",
       "├─Sequential: 1-5                                                 --\n",
       "│    └─Linear: 2-11                                               32,832\n",
       "│    └─ReLU: 2-12                                                 --\n",
       "│    └─Linear: 2-13                                               2,080\n",
       "│    └─ReLU: 2-14                                                 --\n",
       "│    └─Linear: 2-15                                               528\n",
       "│    └─ReLU: 2-16                                                 --\n",
       "│    └─Linear: 2-17                                               136\n",
       "├─Linear: 1-6                                                     45\n",
       "==========================================================================================\n",
       "Total params: 37,729,573\n",
       "Trainable params: 37,729,573\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3ecef4-ea04-4b11-af13-36531fda068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch_labels, batch_tokens, batch_texts = batch['label'], batch['tokens'], batch['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbdab4ad-f02d-43ef-a846-a6d75e8e20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 3\n",
      "PREDICT: 3\n",
      "PREDICT PROBA: [[9.269815927837044e-05, 0.0005633677938021719, 0.021826516836881638, 0.6709147691726685, 0.3066026270389557]]\n"
     ]
    }
   ],
   "source": [
    "item = 25\n",
    "y_hat = model(batch_tokens[item, :].to(device))\n",
    "\n",
    "y_true = batch_labels[item]\n",
    "\n",
    "print(f'LABEL: {y_true.item()}')\n",
    "print(f'PREDICT: {torch.argmax(y_hat)}')\n",
    "print(f'PREDICT PROBA: {F.softmax(y_hat, dim=-1).tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9beae6ba-d388-4286-a267-a603a9f15d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(batch_tokens.to(device))\n",
    "\n",
    "y_true = batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8a3afd3-f1e0-4f78-a0df-f0df5e44deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT BATCH: \n",
      " tensor([3, 1, 2, 4, 2, 2, 4, 3, 1, 1, 2, 4, 3, 4, 4, 1, 4, 0, 4, 3, 4, 2, 2, 0,\n",
      "        4, 3, 2, 0, 3, 1, 1, 0, 0, 2, 2, 3, 1, 3, 3, 2, 3, 2, 3, 2, 2, 4, 1, 3,\n",
      "        3, 3, 2, 3, 4, 2, 4, 3, 3, 1, 0, 0, 0, 2, 1, 3], device='cuda:0')\n",
      "TRUE LABELS: \n",
      " tensor([3, 2, 3, 2, 3, 2, 4, 3, 2, 2, 2, 0, 3, 3, 3, 1, 4, 0, 0, 3, 2, 1, 3, 0,\n",
      "        4, 3, 2, 0, 4, 1, 1, 0, 0, 2, 2, 3, 1, 2, 3, 3, 3, 3, 3, 2, 0, 4, 2, 2,\n",
      "        2, 3, 2, 3, 3, 2, 4, 3, 1, 1, 0, 0, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print('PREDICT BATCH: \\n', torch.argmax(y_hat, dim=-1))\n",
    "\n",
    "print('TRUE LABELS: \\n', y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5072f299-a937-46f0-aefd-0546a1316ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH ACC:  0.59375\n"
     ]
    }
   ],
   "source": [
    "batch_acc = torch.mean(\n",
    "    (torch.argmax(y_hat, dim=-1).to('cpu') == y_true)\n",
    "    .to(torch.float)\n",
    ")\n",
    "print('BATCH ACC: ', batch_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5925d38d-ecdb-4f5e-b831-00410fa24005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad4e6357-2f75-4aac-b311-33fbb87363c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = OneCycleLR(optimizer, max_lr=LR, steps_per_epoch=len(train_loader), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df98da-46c7-454c-9249-d361620ae846",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_eval_list = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8ddedbc-5f1b-4454-a42e-06e7325698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/yepreview_model_.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eab0075c-ac73-4b23-b15f-b57306818b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [04:26<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test = eval_model(model, test_loader)\n",
    "print(acc_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b057d-7e06-4920-b790-c9e2265d1e2c",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- [ ] Olhar as métricas\n",
    "- [ ] Olhar o notebook do chary\n",
    "- [ ] Melhorar o modelo\n",
    "- [ ] Arrumar o notebook\n",
    "- [ ] Fazer uma visualização com o umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e377ed5-3d35-4d3a-9f2f-d584800d4b38",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2df57-a00d-4df7-b575-2c12bad5ada8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
