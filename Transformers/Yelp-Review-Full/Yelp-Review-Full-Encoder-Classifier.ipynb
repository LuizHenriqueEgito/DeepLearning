{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2600ede9-0720-4cbe-b3ac-be5ad9f22cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742e57ca-ae5f-4c45-ac58-9c906bc68180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46350e1e-7804-4008-9881-b51b584714e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e3fad7-a44f-4650-93ee-2c5b26059bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_loop import eval_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a81a480-7d4d-4b73-8c90-e99f711dceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de GPUs disponíveis: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar o número de GPUs disponíveis\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Número de GPUs disponíveis: {num_gpus}\")\n",
    "\n",
    "# Obter informações detalhadas sobre cada GPU\n",
    "for i in range(num_gpus):\n",
    "    print(f\"--- GPU {i} ---\")\n",
    "    print(f\"Nome: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Memória total: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"Memória disponível: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n",
    "    print(f\"Memória reservada: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n",
    "    print(f\"Capacidade de Computação: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4660659-bd52-4878-bb38-a46656d35cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'DEVICE: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9f35c8-4414-444d-b683-30a744a3c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Acessar os dados\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Exibir informações sobre o conjunto de dados\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa90e902-9343-4d39-b932-7afba9cbb7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the text\n",
    "train_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4280f7de-03d3-44bc-a9d9-634fb7a7783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average of texts\n",
    "mean_words_text = np.mean(\n",
    "    list(map(lambda x: len(x.split()), train_data['text']))\n",
    ")\n",
    "\n",
    "std_words_text = np.std(\n",
    "    list(map(lambda x: len(x.split()), train_data['text']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756ff488-763a-47c9-86dd-2a0eb91d2061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de palavras por texto: 134.10.\n",
      "Desvio padrão de palavras por texto: 121.40.\n"
     ]
    }
   ],
   "source": [
    "print(f'Média de palavras por texto: {mean_words_text:.2f}.')\n",
    "print(f'Desvio padrão de palavras por texto: {std_words_text:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b3e1d4b-8878-49ba-9a0a-a32b520df589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6912763e-8927-4ea7-a1fe-3d6b348346d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: \n",
      "After waiting for almost 30 minutes to trade in an old phone part of the buy back program, our customer service rep incorrectly processed the transaction. This led to us waiting another 30 minutes for him to correct it. Don't visit this store if you want pleasant or good service.\n",
      "TOKENS: \n",
      "['after', 'waiting', 'for', 'almost', '30', 'minutes', 'to', 'trade', 'in', 'an', 'old', 'phone', 'part', 'of', 'the', 'buy', 'back', 'program', ',', 'our', 'customer', 'service', 'rep', 'incorrectly', 'processed', 'the', 'transaction', '.', 'this', 'led', 'to', 'us', 'waiting', 'another', '30', 'minutes', 'for', 'him', 'to', 'correct', 'it', '.', 'don', \"'\", 't', 'visit', 'this', 'store', 'if', 'you', 'want', 'pleasant', 'or', 'good', 'service', '.']\n",
      "TOKENS IDS: \n",
      "[101, 2044, 3403, 2005, 2471, 2382, 2781, 2000, 3119, 1999, 2019, 2214, 3042, 2112, 1997, 1996, 4965, 2067, 2565, 1010, 2256, 8013, 2326, 16360, 19721, 13995, 1996, 12598, 1012, 2023, 2419, 2000, 2149, 3403, 2178, 2382, 2781, 2005, 2032, 2000, 6149, 2009, 1012, 2123, 1005, 1056, 3942, 2023, 3573, 2065, 2017, 2215, 8242, 2030, 2204, 2326, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = train_data['text'][13]\n",
    "print(f'TEXT: \\n{text}')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f'TOKENS: \\n{tokens}')\n",
    "tokens_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(f'TOKENS IDS: \\n{tokens_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e93d51-ded0-4e4e-8e0e-017531aafab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256  # nº de tokens de entrada do modelo\n",
    "D_MODEL = 64  # nº de dimensões de embedding\n",
    "N_HEADS = 4  # nº de cabeças utilizadas no multi-head attention\n",
    "Nx = 2  # nº de vezes que é repassado no multi-head attention\n",
    "N_OUTPUT = 5  # nº de classes de saida\n",
    "VOCAB_SIZE = tokenizer.vocab_size  # vocab size\n",
    "LR = 1e-5  # Learning Rate\n",
    "BATCH_SIZE = 32  # Batch Size\n",
    "EPOCHS = 10  # épocas de trainamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27215228-097f-42c3-a4cd-b9f2a52b17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewFullDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data, \n",
    "        tokenizer: Tokenizer = tokenizer, \n",
    "        seq_len: int = SEQ_LEN\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.num_rows\n",
    "        \n",
    "    def __getitem__(self, id_i) -> dict[int, list[int]]:\n",
    "        item = self.data[id_i]\n",
    "        label, text = item['label'], item['text']\n",
    "        tokens_list = tokenizer.encode(\n",
    "            text,\n",
    "            max_length=self.seq_len,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',  # Adiciona padding até max_length\n",
    "            return_tensors='pt' \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'label': label, \n",
    "            'tokens': tokens_list.squeeze(0), \n",
    "            'text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe03329-3dc1-4778-9ef5-a0d5fff4b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YelpReviewFullDataset(train_data)\n",
    "test_dataset = YelpReviewFullDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d39a7e-0865-48ea-9dfe-14122c5b960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7923c6b-1c80-4532-9a8e-353fb2d42377",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac34b8-b8db-4e37-9adb-05a10a4d3776",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa01d143-e997-4fab-8e82-42a622e1eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcc5f249-06cf-4d56-8e08-de3b66617fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YepReviewModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int = SEQ_LEN,\n",
    "        d_model: int = D_MODEL,\n",
    "        vocab_size: int = VOCAB_SIZE,\n",
    "        num_heads: int = N_HEADS,\n",
    "        n_x: int = Nx,\n",
    "        dropout: float = 0.1,\n",
    "        n_outputs: int = N_OUTPUT\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # configurações do modelo\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.n_x = n_x\n",
    "        self.dropout = dropout\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        # componentes\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, \n",
    "            embedding_dim=self.d_model, \n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model, \n",
    "            nhead=self.num_heads,\n",
    "            dropout=self.dropout, \n",
    "            norm_first=True, \n",
    "            batch_first=True,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=self.n_x)\n",
    "        \n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(8, self.n_outputs)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.linear_layer:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.encoder_block(x)\n",
    "        # Pegando a representação vetorial do token <CLS>\n",
    "        x = x[:, 0, :]\n",
    "        x = self.linear_layer(x)\n",
    "        # x = F.layer_norm(x, x.size()[1:])\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec610bc9-eafd-4339-bea3-150a2bf89858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = YepReviewModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5660245f-19a8-40f3-bb6e-43a7ea7faa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load('models/yepreview_model_.pth', map_location=device))\n",
    "    model.to(device)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7975e3b-0edd-41fa-8b3a-9c15e6f57b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "YepReviewModel                                                    --\n",
       "├─Embedding: 1-1                                                  1,953,408\n",
       "├─TransformerEncoderLayer: 1-2                                    --\n",
       "│    └─MultiheadAttention: 2-1                                    12,480\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1                  4,160\n",
       "│    └─Linear: 2-2                                                133,120\n",
       "│    └─Dropout: 2-3                                               --\n",
       "│    └─Linear: 2-4                                                131,136\n",
       "│    └─LayerNorm: 2-5                                             128\n",
       "│    └─LayerNorm: 2-6                                             128\n",
       "│    └─Dropout: 2-7                                               --\n",
       "│    └─Dropout: 2-8                                               --\n",
       "├─TransformerEncoder: 1-3                                         --\n",
       "│    └─ModuleList: 2-9                                            --\n",
       "│    │    └─TransformerEncoderLayer: 3-2                          281,152\n",
       "│    │    └─TransformerEncoderLayer: 3-3                          281,152\n",
       "├─Sequential: 1-4                                                 --\n",
       "│    └─Linear: 2-10                                               4,160\n",
       "│    └─ReLU: 2-11                                                 --\n",
       "│    └─Linear: 2-12                                               2,080\n",
       "│    └─ReLU: 2-13                                                 --\n",
       "│    └─Linear: 2-14                                               528\n",
       "│    └─ReLU: 2-15                                                 --\n",
       "│    └─Linear: 2-16                                               136\n",
       "├─Linear: 1-5                                                     45\n",
       "==========================================================================================\n",
       "Total params: 2,803,813\n",
       "Trainable params: 2,803,813\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad4e6357-2f75-4aac-b311-33fbb87363c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3990d5-0c7f-4337-96a6-ff28ab3536f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 131/20313 [01:28<3:47:27,  1.48it/s, loss=0.9443, accuracy=0.5859]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_eval_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DeepLearning/Transformers/Yelp-Review-Full/utils/training_loop.py:62\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_train, data_eval, epochs, optimizer, criterion, scheduler, save_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m     60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(tokens)\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_eval_list = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0075c-ac73-4b23-b15f-b57306818b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = eval_model(model, test_loader)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b057d-7e06-4920-b790-c9e2265d1e2c",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- [ ] Olhar as métricas\n",
    "- [ ] Olhar o notebook do chary\n",
    "- [ ] Melhorar o modelo\n",
    "- [ ] Arrumar o notebook\n",
    "- [ ] Fazer uma visualização com o umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a995a-3988-47ad-9c22-19cd88ad978a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2424726-32b1-4a5c-87d1-7d8f22647af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef2565-8c84-4be3-84d2-6b21025df6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119eca19-ce4e-43cb-b491-fd70d2618d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c237567-c1c0-476e-9403-da9dd2ef3638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8b368-acdc-4ab8-9dbd-0824309b6380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
