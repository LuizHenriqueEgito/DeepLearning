{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f71798d-13fc-493b-9e88-2945edcd55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dad3c8-3584-4317-a6f6-076d64bc1aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M:\\disco M\\Python\\venvs\\env_torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9c4947b-f37e-49e2-916e-e0cb337b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from utils.text_generation import LMPipeline\n",
    "from utils.transformer_decoder import DecoderLM\n",
    "from utils.tokenizer import MyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4275b7a9-73f3-4409-8ede-4e79b8fb2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b56a73-6272-4339-9b2e-a1752ca20097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "dataset = load_dataset(\"AiresPucrs/google-play-apps-review-pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c020899-e0fc-475d-a65f-7086b4806170",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc8b0fd2-eb09-485d-a277-9bed2aa3798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12_000\n",
    "LR = 5e-5\n",
    "MAX_LEN = 128\n",
    "D_MODEL = 124\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 4\n",
    "HIDDEN_SIZE = 512\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6d80e0-584c-4a69-872c-d0aaf784fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40c1f1b-480e-409c-9ef2-c11508318b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.create_tokenizer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9c9cb7-7d1d-4e5b-952e-451cfd5f1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokeniznado: tensor([  2, 857,   3])\n",
      "Detokenizando:  ola \n"
     ]
    }
   ],
   "source": [
    "print(f'Tokeniznado: {tokenizer.tokenize_text(\"ola\")}')\n",
    "print(f'Detokenizando: {tokenizer.untokenize_tokens([2 ,857, 3])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e7cf7f-cd0e-4ab4-909d-9d1732d42de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11465"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab_transform)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efccbf6-91f8-4cc8-a234-f80c43b7d6c1",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3424064-75f7-44d9-9819-9a524ef1d30a",
   "metadata": {},
   "source": [
    "- COLOQUE O \\<EOS>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9743d91-cee9-4c52-bf0b-816e55337017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a346cf-8dee-4e54-8585-279150ea5f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o aplicativo e bom disparadamente melhor que o concorrente whatsapp pontos positivos  possibilidade de aplicar temas personalizados para sair da aparencia padrao de acordo com o usuario  a nao utilizacao de um backup local e sem a possibilidade de perder todas as mensagens acidentalmente por ser um servico via nuvem  a possibilidade de usar bots como um diferencial alem de somente usar o aplicativo para conversar ou seja e possivel ampliar o uso do aplicativo para outras coisas interessantes como por exemplo estudar  a possibilidade de se entreter com jogos e se divertir com outros contatosamigos similar ao ponto anterior  a existencia de um chat secreto para autodestruir mensagens que 2 usuarios nao queiram que fiquem armazenadas na nuvem sendo assim uma forma de conversar com privacidade total ainda ha outros pontos positivos mas nao e necessario citar todos eu tenho somente um ponto negativo tal ponto e a instabilidade do sistema em nuvem do telegram que certas vezes dessincroniza as mensagens enviadas e recebidas eu tambem sugiro adicionar a funcao de esconder a foto de perfil para contatosninguem e uma contagem de mensagens'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5ce8c5-578c-4215-9223-9069bd1464c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        tokenizer: MyTokenizer,\n",
    "        max_len: int = MAX_LEN,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = len(self.tokenizer.vocab_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset['train'].num_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataset['train'][index]['review']\n",
    "        tokenized = self.tokenizer.tokenize_text(text)\n",
    "        if len(tokenized) < self.max_len + 1:\n",
    "            tokenized = tokenized.tolist()\n",
    "            # porque colocar o <pad> antes do texto?\n",
    "            tokenized = [self.tokenizer.PAD_IDX] * ((self.max_len + 1) - len(tokenized)) + tokenized\n",
    "            tokenized = torch.tensor(tokenized)\n",
    "        else:\n",
    "            tokenized = tokenized[:self.max_len + 1]\n",
    "\n",
    "        decoder_input = tokenized[: self.max_len]\n",
    "        true_output = tokenized[1 : self.max_len + 1]\n",
    "        # return decoder_input, true_output\n",
    "        return {'decoder_input': decoder_input, 'true_output': true_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93e6378e-7ab1-45ae-b274-f4dd71d4e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GoogleDataset(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b705d6a4-7548-4a6d-ba1e-7308f630efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O que vai entrar no modelo: \n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    2,   26,  528,   68,   21,  622,   19,  200,\n",
      "           8,  192,  171,    5,   19,   10,   44,  622,    5,   10, 1017,    8,\n",
      "         677,  428,  317, 1185,    7, 1167,   28,  777,    7,  225,   10,  256,\n",
      "        1167,    5,   10,    8,   34,    5, 6690,   40, 1167,   20,  689,   80,\n",
      "         385,  428,   12,  699,  216,    7, 1167, 1167,   20, 1244,  412,  216,\n",
      "          10,  135,    8,    5, 4148,   30, 1483,   13, 9495, 2528,    6, 1547,\n",
      "          43,   10,  260,   20, 2556,    5,  363, 1364,   10, 1206,   13,  129,\n",
      "         564,    8,   58,  221, 1972, 7307, 1989,   15,   10, 1217,  203, 1757,\n",
      "          12, 1525,   18, 4297,    5, 1986,  339, 2493])\n",
      "******************************\n",
      "O que deve sair do modelo: \n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    2,   26,  528,   68,   21,  622,   19,  200,    8,\n",
      "         192,  171,    5,   19,   10,   44,  622,    5,   10, 1017,    8,  677,\n",
      "         428,  317, 1185,    7, 1167,   28,  777,    7,  225,   10,  256, 1167,\n",
      "           5,   10,    8,   34,    5, 6690,   40, 1167,   20,  689,   80,  385,\n",
      "         428,   12,  699,  216,    7, 1167, 1167,   20, 1244,  412,  216,   10,\n",
      "         135,    8,    5, 4148,   30, 1483,   13, 9495, 2528,    6, 1547,   43,\n",
      "          10,  260,   20, 2556,    5,  363, 1364,   10, 1206,   13,  129,  564,\n",
      "           8,   58,  221, 1972, 7307, 1989,   15,   10, 1217,  203, 1757,   12,\n",
      "        1525,   18, 4297,    5, 1986,  339, 2493,    3])\n"
     ]
    }
   ],
   "source": [
    "# print(f'O que vai entrar no modelo: \\n{train_dataset[12][0]}')\n",
    "print(f'O que vai entrar no modelo: \\n{train_dataset[12][\"decoder_input\"]}')\n",
    "print('*'*30)\n",
    "# print(f'O que deve sair do modelo: \\n{train_dataset[12][1]}')\n",
    "print(f'O que deve sair do modelo: \\n{train_dataset[12][\"true_output\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98484808-6170-46b3-97d1-85daa50cdabb",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1362aa4-c905-4a55-89b2-3c4c01be8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03bde07-51ee-4af5-ab16-917b0e221139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=D_MODEL, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, device=device)   \n",
    "        position = torch.arange(start=0, end=max_len, step=1, device=device).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2, device=device).float()\n",
    "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, word_embeddings):\n",
    "    \n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e778e558-7437-44da-a20f-7091e57495f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): \n",
    "    \n",
    "    def __init__(self, d_model=D_MODEL):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model=d_model\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False).to(device)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False).to(device)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False).to(device)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "        \n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb4d1a4e-cce1-4f79-8c84-755ef5d4be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_tokens=VOCAB_SIZE, d_model=D_MODEL, max_len=MAX_LEN):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, \n",
    "                               embedding_dim=d_model).to(device) \n",
    "        self.pe = PositionEncoding(d_model=d_model, \n",
    "                                   max_len=max_len).to(device)\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens).to(device)\n",
    "\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "                \n",
    "        word_embeddings = self.we(token_ids.to(device))        \n",
    "        position_encoded = self.pe(word_embeddings.to(device))\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=device))\n",
    "        mask = mask == 0\n",
    "        \n",
    "        self_attention_values = self.self_attention(position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    position_encoded, \n",
    "                                                    mask=mask)\n",
    "                \n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "        \n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dab3881a-5e67-4ace-95bc-1d9bf7fd7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecoderOnlyTransformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aaa59389-f0d8-4f8c-a55f-87591e4323ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f408fad7-ed1c-4bc4-b4df-5de26735864a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DecoderOnlyTransformer                   --\n",
       "├─Embedding: 1-1                         1,488,000\n",
       "├─PositionEncoding: 1-2                  --\n",
       "├─Attention: 1-3                         --\n",
       "│    └─Linear: 2-1                       15,376\n",
       "│    └─Linear: 2-2                       15,376\n",
       "│    └─Linear: 2-3                       15,376\n",
       "├─Linear: 1-4                            1,500,000\n",
       "=================================================================\n",
       "Total params: 3,034,128\n",
       "Trainable params: 3,034,128\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c4eb748-d84d-467f-ad6a-7e5e5ec6f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a52a68c-66a1-4e8c-b131-9021c9548322",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    train_dataset, num_workers=0, shuffle=True, batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3997a506-63be-4dfd-b0d1-a81ab20623d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca: 0\tUpdate: 20000\tLoss: 2.5496\tAccum_loss: 3.0476: 100%|████████████████████| 20000/20000 [04:35<00:00, 72.50it/s]\n",
      "Epoca: 1\tUpdate: 21194\tLoss: 3.1353\tAccum_loss: 2.9224:   6%|█▎                   | 1194/20000 [00:15<04:11, 74.90it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "accum_loss = None\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "for epoca in range(EPOCHS):\n",
    "    batch_iterator = tqdm(dataloader, desc=f\"Processing Epoch {epoca:02d}\")\n",
    "    for batch in batch_iterator:\n",
    "        x, y = batch['decoder_input'], batch['true_output']\n",
    "        x = x.to(device)\n",
    "        y = y.long().to(device)\n",
    "        y_hat = model(x[0])\n",
    "        loss = F.cross_entropy(y_hat, y[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if accum_loss is not None:\n",
    "            accum_loss = 0.99 * accum_loss + 0.01 * loss.detach().cpu().item()\n",
    "        else:\n",
    "            accum_loss = loss.detach().cpu().item()\n",
    "\n",
    "        update_count += 1\n",
    "        batch_iterator.set_description(\n",
    "            f\"Epoca: {epoca}\\tUpdate: {update_count}\\tLoss: {loss.detach().cpu().item():.4f}\\tAccum_loss: {accum_loss:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48856430-b82f-4d8d-a986-f85ea3b8e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\t \n"
     ]
    }
   ],
   "source": [
    "model_input = x\n",
    "input_length = VOCAB_SIZE\n",
    "\n",
    "predictions = model(model_input[0]) \n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "for i in range(input_length, 1000):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
    "        break\n",
    "    \n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    \n",
    "    predictions = model(model_input) \n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "        \n",
    "print(\"Predicted Tokens:\") \n",
    "for id_ in predicted_ids: \n",
    "    print(\"\\t\", tokenizer.untokenize_tokens([id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75275246-1c93-4924-8fd2-74b018ed4bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    2,  103,   39,   58,   26,   17,  555,   33,\n",
       "           25,  552,  291,   22,  178,   20,  115,   10,  433,   77,  151,  118,\n",
       "           14,  380,  257,   96, 2950, 3454,   10,   67,    9, 6313,    6,  161,\n",
       "           10,  309,   34,  326, 1143,    5,    9, 3361,    6,    8, 1552,   17,\n",
       "          406,   64,  860,    1,   50,   17,   11,   10,   54,  321,  161,  833,\n",
       "            6,  101,  130,  694,   94, 4491,   12,  222]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "840c1285-41cb-4514-adec-f8b5c2bc852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    2,  311,   64,  860,   14,    6,  514,   98,   21,  113,   17,\n",
       "         2107, 9504,    5,    9,   48,  518,   22,  573, 3468,  183,  169,   20,\n",
       "          129,   14,    6,  307,    5,    9,  214, 1684,    8,    6,  307,    9,\n",
       "          351,  501,   20,  129,   14,   10,  879,    7,  229,   13,  514,   15,\n",
       "          440,   43,   57,   46, 4584,  258,    6,  292,   57,    6,  328,    9,\n",
       "           89,  399,   61,   21,  147,  258,    6,   36,  292,   31,  485,    6,\n",
       "          335,  102,  599, 2717,  339,  401,  480,    3]], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed85b02f-38ed-438e-aba4-d236b8575a13",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mM:\\disco M\\Python\\venvs\\env_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[39], line 16\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids):\n\u001b[1;32m---> 16\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     17\u001b[0m     position_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(word_embeddings)\n\u001b[0;32m     18\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((token_ids\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), token_ids\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mM:\\disco M\\Python\\venvs\\env_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mM:\\disco M\\Python\\venvs\\env_torch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mM:\\disco M\\Python\\venvs\\env_torch\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "model(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83b66b-8e8e-4d0f-a0bc-6ad8ea9910ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "env_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
