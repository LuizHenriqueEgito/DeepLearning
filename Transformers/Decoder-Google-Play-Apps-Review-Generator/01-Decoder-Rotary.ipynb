{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16038acf-4e01-4677-a637-c1daf5342019",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a31cf8-66a5-4649-952a-d897be31f455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M:\\disco M\\Python\\venvs\\env_torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db3cd1fa-2863-4af7-8af6-58ac6987c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from utils.text_generation import LMPipeline\n",
    "from utils.transformer_decoder import DecoderLM\n",
    "from utils.tokenizer import MyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d34653-6ca0-48fa-a44e-69a03b138169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "dataset = load_dataset(\"AiresPucrs/google-play-apps-review-pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8030524d-40a1-479b-baed-f68ae0b744cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7508396-6693-4283-9f61-c2394714b86d",
   "metadata": {},
   "source": [
    "# Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0f060fd-1f6c-4925-bf84-6c044e04fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12_000\n",
    "MAX_LEN = 128\n",
    "D_MODEL = 124\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 4\n",
    "HIDDEN_SIZE = 512\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2090ca-cb22-4f3c-b376-126a20735e25",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9d910b-9fe3-4070-8048-17da4151da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b8f7e6-f77e-4017-a3e3-42c5c60019ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.create_tokenizer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9efe0c91-f64b-442c-b055-54c36325ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.vocab_transform.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b12c8fd-ba8d-4d6a-a3d8-a8571c624786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokeniznado: tensor([  2, 857,   3])\n",
      "Detokenizando:  ola \n"
     ]
    }
   ],
   "source": [
    "print(f'Tokeniznado: {tokenizer.tokenize_text(\"ola\")}')\n",
    "print(f'Detokenizando: {tokenizer.untokenize_tokens([2 ,857, 3])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeeb3e76-97b2-439a-9187-6b124bbf0f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11465"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab_transform)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d512f53-2bb7-44a8-a1d6-90be16a13008",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e93e5a1-97c2-4aea-8701-408927988fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe637d3a-5551-4c13-8072-8190260bff16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ed7a2d-e9a2-425f-bf33-50235d377fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o aplicativo e bom disparadamente melhor que o concorrente whatsapp pontos positivos  possibilidade de aplicar temas personalizados para sair da aparencia padrao de acordo com o usuario  a nao utilizacao de um backup local e sem a possibilidade de perder todas as mensagens acidentalmente por ser um servico via nuvem  a possibilidade de usar bots como um diferencial alem de somente usar o aplicativo para conversar ou seja e possivel ampliar o uso do aplicativo para outras coisas interessantes como por exemplo estudar  a possibilidade de se entreter com jogos e se divertir com outros contatosamigos similar ao ponto anterior  a existencia de um chat secreto para autodestruir mensagens que 2 usuarios nao queiram que fiquem armazenadas na nuvem sendo assim uma forma de conversar com privacidade total ainda ha outros pontos positivos mas nao e necessario citar todos eu tenho somente um ponto negativo tal ponto e a instabilidade do sistema em nuvem do telegram que certas vezes dessincroniza as mensagens enviadas e recebidas eu tambem sugiro adicionar a funcao de esconder a foto de perfil para contatosninguem e uma contagem de mensagens'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8cdd41-25ed-40b8-ae57-45044500b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        tokenizer: MyTokenizer,\n",
    "        max_len: int = MAX_LEN,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = len(self.tokenizer.vocab_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset['train'].num_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataset['train'][index]['review']\n",
    "        tokenized = self.tokenizer.tokenize_text(text)\n",
    "        if len(tokenized) < self.max_len + 1:\n",
    "            tokenized = tokenized.tolist()\n",
    "            # porque colocar o <pad> antes do texto?\n",
    "            tokenized = [self.tokenizer.PAD_IDX] * ((self.max_len + 1) - len(tokenized)) + tokenized\n",
    "            tokenized = torch.tensor(tokenized)\n",
    "        else:\n",
    "            tokenized = tokenized[:self.max_len + 1]\n",
    "\n",
    "        decoder_input = tokenized[: self.max_len]\n",
    "        true_output = tokenized[1 : self.max_len + 1]\n",
    "        # return decoder_input, true_output\n",
    "        return {'decoder_input': decoder_input, 'true_output': true_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dfdb8be-7c58-4d8e-abfa-e73c5e6b3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GoogleDataset(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eef2fe70-5519-4f11-ad02-8101e4f6114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O que vai entrar no modelo: \n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    2,   26,  528,   68,   21,  622,   19,  200,\n",
      "           8,  192,  171,    5,   19,   10,   44,  622,    5,   10, 1017,    8,\n",
      "         677,  428,  317, 1185,    7, 1167,   28,  777,    7,  225,   10,  256,\n",
      "        1167,    5,   10,    8,   34,    5, 6690,   40, 1167,   20,  689,   80,\n",
      "         385,  428,   12,  699,  216,    7, 1167, 1167,   20, 1244,  412,  216,\n",
      "          10,  135,    8,    5, 4148,   30, 1483,   13, 9495, 2528,    6, 1547,\n",
      "          43,   10,  260,   20, 2556,    5,  363, 1364,   10, 1206,   13,  129,\n",
      "         564,    8,   58,  221, 1972, 7307, 1989,   15,   10, 1217,  203, 1757,\n",
      "          12, 1525,   18, 4297,    5, 1986,  339, 2493])\n",
      "******************************\n",
      "O que deve sair do modelo: \n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    2,   26,  528,   68,   21,  622,   19,  200,    8,\n",
      "         192,  171,    5,   19,   10,   44,  622,    5,   10, 1017,    8,  677,\n",
      "         428,  317, 1185,    7, 1167,   28,  777,    7,  225,   10,  256, 1167,\n",
      "           5,   10,    8,   34,    5, 6690,   40, 1167,   20,  689,   80,  385,\n",
      "         428,   12,  699,  216,    7, 1167, 1167,   20, 1244,  412,  216,   10,\n",
      "         135,    8,    5, 4148,   30, 1483,   13, 9495, 2528,    6, 1547,   43,\n",
      "          10,  260,   20, 2556,    5,  363, 1364,   10, 1206,   13,  129,  564,\n",
      "           8,   58,  221, 1972, 7307, 1989,   15,   10, 1217,  203, 1757,   12,\n",
      "        1525,   18, 4297,    5, 1986,  339, 2493,    3])\n"
     ]
    }
   ],
   "source": [
    "# print(f'O que vai entrar no modelo: \\n{train_dataset[12][0]}')\n",
    "print(f'O que vai entrar no modelo: \\n{train_dataset[12][\"decoder_input\"]}')\n",
    "print('*'*30)\n",
    "# print(f'O que deve sair do modelo: \\n{train_dataset[12][1]}')\n",
    "print(f'O que deve sair do modelo: \\n{train_dataset[12][\"true_output\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212464b8-7f38-479a-8777-86ba86d1e9ba",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3a76abf-6b3c-48d4-980d-babf7815aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from utils.transformer import TransformerDecoder\n",
    "\n",
    "\n",
    "class DecoderLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_len=MAX_LEN,\n",
    "        embed_dim=D_MODEL,\n",
    "        num_layers=N_LAYERS,\n",
    "        num_heads=N_HEADS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        dropout=DROPOUT,\n",
    "        pad_token_id=tokenizer.PAD_IDX,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            max_len=max_len,\n",
    "            embed_dim=embed_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, non_pad_indexes=None):\n",
    "        # we dont want to compute loss for padding tokens\n",
    "        # get all hidden states\n",
    "        logits = self.lm_head(self.decoder(x))\n",
    "        # remove batch dimension\n",
    "        logits = torch.reshape(logits, (-1, self.vocab_size))\n",
    "        # get only the tokens that matter\n",
    "        if non_pad_indexes is not None:\n",
    "            logits = logits[non_pad_indexes, :]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "652f3a1a-7ebe-4199-8cef-4581cc438778",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = DecoderLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "685dc78c-c544-4fe0-bf2f-d6e869fe6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "DecoderLM                                     --\n",
       "├─TransformerDecoder: 1-1                     --\n",
       "│    └─Embedding: 2-1                         1,488,000\n",
       "│    └─PositionalEncoding: 2-2                --\n",
       "│    │    └─Dropout: 3-1                      --\n",
       "│    └─ModuleList: 2-3                        --\n",
       "│    │    └─DecoderBlock: 3-2                 190,108\n",
       "│    │    └─DecoderBlock: 3-3                 190,108\n",
       "│    │    └─DecoderBlock: 3-4                 190,108\n",
       "│    │    └─DecoderBlock: 3-5                 190,108\n",
       "│    │    └─DecoderBlock: 3-6                 190,108\n",
       "│    │    └─DecoderBlock: 3-7                 190,108\n",
       "│    └─LayerNorm: 2-4                         248\n",
       "├─Linear: 1-2                                 1,500,000\n",
       "======================================================================\n",
       "Total params: 4,128,896\n",
       "Trainable params: 4,128,896\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5da4dab-af06-42f9-b0e2-5e5a19cc53cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLM(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(12000, 124, padding_idx=0)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (out_projection): Linear(in_features=124, out_features=124, bias=True)\n",
       "          (proj_q): Linear(in_features=124, out_features=124, bias=True)\n",
       "          (proj_k): Linear(in_features=124, out_features=124, bias=True)\n",
       "          (proj_v): Linear(in_features=124, out_features=124, bias=True)\n",
       "          (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "          (dropout_projection): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): FeedFowardBlock(\n",
       "          (ff_1): Linear(in_features=124, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ff_2): Linear(in_features=512, out_features=124, bias=True)\n",
       "          (activation): NewGELU()\n",
       "        )\n",
       "        (norm_1): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (last_norm): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=124, out_features=12000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a4c119e-91ba-4649-943f-8a105a21ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(modelo.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30ac13e8-e7f1-4644-9bfa-81ab9b899dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    train_dataset, num_workers=0, shuffle=True, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05d3c9d2-33a3-4010-88c2-295757071921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca: 0\tUpdate: 2500\tLoss: 4.6935\tAccum_loss: 4.5727: 100%|███████████████████████| 2500/2500 [01:37<00:00, 25.70it/s]\n",
      "Epoca: 1\tUpdate: 5000\tLoss: 4.5517\tAccum_loss: 4.5292: 100%|███████████████████████| 2500/2500 [01:41<00:00, 24.52it/s]\n",
      "Epoca: 2\tUpdate: 7500\tLoss: 4.6824\tAccum_loss: 4.4645: 100%|███████████████████████| 2500/2500 [01:41<00:00, 24.63it/s]\n",
      "Epoca: 3\tUpdate: 10000\tLoss: 4.6143\tAccum_loss: 4.4545: 100%|██████████████████████| 2500/2500 [01:41<00:00, 24.55it/s]\n",
      "Epoca: 4\tUpdate: 12500\tLoss: 4.4253\tAccum_loss: 4.4196: 100%|██████████████████████| 2500/2500 [01:42<00:00, 24.30it/s]\n",
      "Epoca: 5\tUpdate: 15000\tLoss: 4.3295\tAccum_loss: 4.3856: 100%|██████████████████████| 2500/2500 [01:41<00:00, 24.67it/s]\n",
      "Epoca: 6\tUpdate: 17500\tLoss: 4.4416\tAccum_loss: 4.3616: 100%|██████████████████████| 2500/2500 [01:41<00:00, 24.66it/s]\n",
      "Epoca: 7\tUpdate: 20000\tLoss: 4.1651\tAccum_loss: 4.3385: 100%|██████████████████████| 2500/2500 [01:41<00:00, 24.70it/s]\n",
      "Epoca: 8\tUpdate: 22500\tLoss: 4.5494\tAccum_loss: 4.3124: 100%|██████████████████████| 2500/2500 [01:40<00:00, 24.90it/s]\n",
      "Epoca: 9\tUpdate: 25000\tLoss: 4.1681\tAccum_loss: 4.2806: 100%|██████████████████████| 2500/2500 [01:39<00:00, 25.12it/s]\n",
      "Epoca: 10\tUpdate: 27500\tLoss: 4.1489\tAccum_loss: 4.2277: 100%|█████████████████████| 2500/2500 [01:39<00:00, 25.14it/s]\n",
      "Epoca: 11\tUpdate: 30000\tLoss: 4.2220\tAccum_loss: 4.2182: 100%|█████████████████████| 2500/2500 [01:38<00:00, 25.40it/s]\n",
      "Epoca: 12\tUpdate: 32500\tLoss: 4.3345\tAccum_loss: 4.1989: 100%|█████████████████████| 2500/2500 [01:39<00:00, 25.19it/s]\n",
      "Epoca: 13\tUpdate: 35000\tLoss: 4.0805\tAccum_loss: 4.1744: 100%|█████████████████████| 2500/2500 [01:38<00:00, 25.32it/s]\n",
      "Epoca: 14\tUpdate: 37500\tLoss: 3.9730\tAccum_loss: 4.1514: 100%|█████████████████████| 2500/2500 [01:37<00:00, 25.64it/s]\n",
      "Epoca: 15\tUpdate: 40000\tLoss: 4.1315\tAccum_loss: 4.1038: 100%|█████████████████████| 2500/2500 [01:37<00:00, 25.58it/s]\n",
      "Epoca: 16\tUpdate: 42500\tLoss: 4.0941\tAccum_loss: 4.0929: 100%|█████████████████████| 2500/2500 [01:37<00:00, 25.71it/s]\n",
      "Epoca: 17\tUpdate: 45000\tLoss: 4.3459\tAccum_loss: 4.0765: 100%|█████████████████████| 2500/2500 [01:37<00:00, 25.75it/s]\n",
      "Epoca: 18\tUpdate: 47500\tLoss: 4.2358\tAccum_loss: 4.0300: 100%|█████████████████████| 2500/2500 [01:36<00:00, 25.91it/s]\n",
      "Epoca: 19\tUpdate: 50000\tLoss: 4.2692\tAccum_loss: 4.0406: 100%|█████████████████████| 2500/2500 [01:35<00:00, 26.24it/s]\n",
      "Epoca: 20\tUpdate: 52500\tLoss: 3.6900\tAccum_loss: 4.0033: 100%|█████████████████████| 2500/2500 [01:35<00:00, 26.31it/s]\n",
      "Epoca: 21\tUpdate: 55000\tLoss: 4.2493\tAccum_loss: 3.9819: 100%|█████████████████████| 2500/2500 [01:34<00:00, 26.43it/s]\n",
      "Epoca: 22\tUpdate: 57500\tLoss: 3.9097\tAccum_loss: 3.9568: 100%|█████████████████████| 2500/2500 [01:34<00:00, 26.36it/s]\n",
      "Epoca: 23\tUpdate: 60000\tLoss: 3.9337\tAccum_loss: 3.9563: 100%|█████████████████████| 2500/2500 [01:36<00:00, 25.79it/s]\n",
      "Epoca: 24\tUpdate: 62500\tLoss: 3.7850\tAccum_loss: 3.9497: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.62it/s]\n",
      "Epoca: 25\tUpdate: 65000\tLoss: 4.2328\tAccum_loss: 3.8990: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.64it/s]\n",
      "Epoca: 26\tUpdate: 67500\tLoss: 3.8822\tAccum_loss: 3.8980: 100%|█████████████████████| 2500/2500 [01:34<00:00, 26.35it/s]\n",
      "Epoca: 27\tUpdate: 70000\tLoss: 3.9851\tAccum_loss: 3.8751: 100%|█████████████████████| 2500/2500 [01:35<00:00, 26.12it/s]\n",
      "Epoca: 28\tUpdate: 72500\tLoss: 3.9493\tAccum_loss: 3.8503: 100%|█████████████████████| 2500/2500 [01:34<00:00, 26.38it/s]\n",
      "Epoca: 29\tUpdate: 75000\tLoss: 3.8758\tAccum_loss: 3.8211: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.81it/s]\n",
      "Epoca: 30\tUpdate: 77500\tLoss: 3.6641\tAccum_loss: 3.8158: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.73it/s]\n",
      "Epoca: 31\tUpdate: 80000\tLoss: 3.6352\tAccum_loss: 3.7752: 100%|█████████████████████| 2500/2500 [01:34<00:00, 26.57it/s]\n",
      "Epoca: 32\tUpdate: 82500\tLoss: 3.7637\tAccum_loss: 3.7842: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.60it/s]\n",
      "Epoca: 33\tUpdate: 85000\tLoss: 3.7608\tAccum_loss: 3.7382: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.75it/s]\n",
      "Epoca: 34\tUpdate: 87500\tLoss: 3.9116\tAccum_loss: 3.7537: 100%|█████████████████████| 2500/2500 [01:32<00:00, 26.94it/s]\n",
      "Epoca: 35\tUpdate: 90000\tLoss: 3.9033\tAccum_loss: 3.7162: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.78it/s]\n",
      "Epoca: 36\tUpdate: 92500\tLoss: 3.8530\tAccum_loss: 3.7207: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.66it/s]\n",
      "Epoca: 37\tUpdate: 95000\tLoss: 3.5978\tAccum_loss: 3.6868: 100%|█████████████████████| 2500/2500 [01:33<00:00, 26.72it/s]\n",
      "Epoca: 38\tUpdate: 97500\tLoss: 3.9351\tAccum_loss: 3.6745: 100%|█████████████████████| 2500/2500 [01:32<00:00, 27.01it/s]\n",
      "Epoca: 39\tUpdate: 100000\tLoss: 3.4845\tAccum_loss: 3.6597: 100%|████████████████████| 2500/2500 [01:32<00:00, 26.96it/s]\n",
      "Epoca: 40\tUpdate: 102500\tLoss: 3.4566\tAccum_loss: 3.6387: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.73it/s]\n",
      "Epoca: 41\tUpdate: 105000\tLoss: 3.2706\tAccum_loss: 3.6127: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.72it/s]\n",
      "Epoca: 42\tUpdate: 107500\tLoss: 3.6877\tAccum_loss: 3.6042: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.76it/s]\n",
      "Epoca: 43\tUpdate: 110000\tLoss: 3.6826\tAccum_loss: 3.6005: 100%|████████████████████| 2500/2500 [01:32<00:00, 26.98it/s]\n",
      "Epoca: 44\tUpdate: 112500\tLoss: 3.5163\tAccum_loss: 3.5667: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.71it/s]\n",
      "Epoca: 45\tUpdate: 115000\tLoss: 3.1846\tAccum_loss: 3.5573: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.65it/s]\n",
      "Epoca: 46\tUpdate: 117500\tLoss: 3.7270\tAccum_loss: 3.5495: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.65it/s]\n",
      "Epoca: 47\tUpdate: 120000\tLoss: 3.6152\tAccum_loss: 3.5299: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.74it/s]\n",
      "Epoca: 48\tUpdate: 122500\tLoss: 3.3723\tAccum_loss: 3.5081: 100%|████████████████████| 2500/2500 [01:32<00:00, 26.99it/s]\n",
      "Epoca: 49\tUpdate: 125000\tLoss: 3.6708\tAccum_loss: 3.4968: 100%|████████████████████| 2500/2500 [01:33<00:00, 26.64it/s]\n"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "accum_loss = None\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "for epoca in range(EPOCHS):\n",
    "    batch_iterator = tqdm(dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    for batch in batch_iterator:\n",
    "        x, y = batch['decoder_input'], batch['true_output']\n",
    "        x = x.to(device)\n",
    "        y = y.long().to(device)\n",
    "\n",
    "        pad_mask = (x != 0).type(torch.int).reshape((-1,))\n",
    "        non_pad_indexes = torch.flatten(pad_mask.nonzero())\n",
    "\n",
    "        # also flat the labels\n",
    "        labels = y.reshape((-1,)).type(torch.long)\n",
    "        labels = labels[non_pad_indexes]\n",
    "        # runs through the model\n",
    "        out = modelo(x, non_pad_indexes)\n",
    "        loss = torch.nn.functional.cross_entropy(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if accum_loss is not None:\n",
    "            accum_loss = 0.99 * accum_loss + 0.01 * loss.detach().cpu().item()\n",
    "        else:\n",
    "            accum_loss = loss.detach().cpu().item()\n",
    "\n",
    "        update_count += 1\n",
    "        batch_iterator.set_description(\n",
    "            f\"Epoca: {epoca}\\tUpdate: {update_count}\\tLoss: {loss.detach().cpu().item():.4f}\\tAccum_loss: {accum_loss:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b668a0d-d3ed-4353-90d2-f3a71182cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " nao gostei do aplicativo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nao gostei do aplicativo  e bem mais nao foi que o app de boa minha conta no meu <unk> por isso e sinceramente ele deveria ter a opcao de editar como antes mas se fosse me comunicar com <unk> eu ja escolhia desta forma para o telegram pois uso desse aplicativo por um motivo tipo de conversas da pessoa que estava dando as mensagens antigas so tenho contatos amigos ou grupos a mensagem ela nunca mais achei pessimo muito chato sendo q era possivel criar uma amiga novas culturas entao porfavor do tinder faz essas coisas que ainda tem me ajudar nisso ta de\n",
      "**************************************************\n",
      " nao gostei do aplicativo  e verdade que nao gostei fiz uma compra pelo item q entrega compra por falta de restaurantes ok mas o app esta com pagamento em dinheiro sou cancelar meu cartao de debito e recebi no mbway porque fiz um reembolso pago a resposta fornecedora e com o uber eats ja que nao tiveram suporte do aplicativo pra resolver fiz contato nunca mais de entrar pelo email hoje eu fiz tudo p fazer pedido e nem   do ifood da 99   fui cliente e mesmo assim esse comportamento e pra ver se deu algo errado tento me ajudar os motoristas\n"
     ]
    }
   ],
   "source": [
    "generation_pipeline = LMPipeline(\n",
    "    mask_token_id=4, tokenizer=tokenizer, model=modelo, sos_token=1, eos_token=2\n",
    ")\n",
    "\n",
    "# input_text = \"\"\"Eu só tenho uma coisa a dizer sobre este produto\"\"\"\n",
    "input_text = input()\n",
    "# input_text = 'o aplicativo'\n",
    "\n",
    "texto_gerado = generation_pipeline.decoder_standard_generation(\n",
    "    input_text=input_text,\n",
    "    max_tokens=100,\n",
    "    decoder_max_len=MAX_LEN,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=20,\n",
    "    num_breams=2,\n",
    "    repetition_penalty=1.2,\n",
    "    device=device,\n",
    ")\n",
    "print(texto_gerado)\n",
    "print(\"*\" * 50)\n",
    "\n",
    "texto_gerado = generation_pipeline.decoder_nucleus_generation(\n",
    "    input_text=input_text,\n",
    "    max_tokens=100,\n",
    "    decoder_max_len=MAX_LEN,\n",
    "    p=0.95,\n",
    "    temperature=0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(texto_gerado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebf967-80b6-4b9a-b20d-9801d410dfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096232d-1d26-4b6e-90ad-948a2c825eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aa116-6a96-494a-96ff-c594941ec9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2227a-3874-4fa4-b1b6-0e335e386c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
